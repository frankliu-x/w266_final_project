{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f988f7b1908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (BertConfig, BertForTokenClassification,\n",
    "                                  BertTokenizer)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_datasets_and_vocabs\n",
    "from model import (Aspect_Text_GAT_ours,\n",
    "                    Pure_Bert, Aspect_Bert_GAT, Aspect_Text_GAT_only)\n",
    "from trainer import train\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument('--dataset_name', type=str, default='rest',\n",
    "                        choices=['rest', 'laptop', 'twitter'],\n",
    "                        help='Choose absa dataset.')\n",
    "    parser.add_argument('--output_dir', type=str, default='/data1/SHENWZH/ABSA_online/data/output-gcn',\n",
    "                        help='Directory to store intermedia data, such as vocab, embeddings, tags_vocab.')\n",
    "    parser.add_argument('--num_classes', type=int, default=3,\n",
    "                        help='Number of classes of ABSA.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--cuda_id', type=str, default='3',\n",
    "                        help='Choose which GPUs to run')\n",
    "    parser.add_argument('--seed', type=int, default=2019,\n",
    "                        help='random seed for initialization')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--glove_dir', type=str, default='/data1/SHENWZH/wordvec',\n",
    "                        help='Directory storing glove embeddings')\n",
    "    parser.add_argument('--bert_model_dir', type=str, default='/data1/SHENWZH/models/bert_base',\n",
    "                        help='Path to pre-trained Bert model.')\n",
    "    parser.add_argument('--pure_bert', action='store_true',\n",
    "                        help='Cat text and aspect, [cls] to predict.')\n",
    "    parser.add_argument('--gat_bert', action='store_true',\n",
    "                        help='Cat text and aspect, [cls] to predict.')\n",
    "\n",
    "    parser.add_argument('--highway', action='store_true',\n",
    "                        help='Use highway embed.')\n",
    "\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='Number of layers of bilstm or highway or elmo.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--add_non_connect',  type= bool, default=True,\n",
    "                        help='Add a sepcial \"non-connect\" relation for aspect with no direct connection.')\n",
    "    parser.add_argument('--multi_hop',  type= bool, default=True,\n",
    "                        help='Multi hop non connection.')\n",
    "    parser.add_argument('--max_hop', type = int, default=4,\n",
    "                        help='max number of hops')\n",
    "\n",
    "\n",
    "    parser.add_argument('--num_heads', type=int, default=6,\n",
    "                        help='Number of heads for gat.')\n",
    "    \n",
    "    parser.add_argument('--dropout', type=float, default=0,\n",
    "                        help='Dropout rate for embedding.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--num_gcn_layers', type=int, default=1,\n",
    "                        help='Number of GCN layers.')\n",
    "    parser.add_argument('--gcn_mem_dim', type=int, default=300,\n",
    "                        help='Dimension of the W in GCN.')\n",
    "    parser.add_argument('--gcn_dropout', type=float, default=0.2,\n",
    "                        help='Dropout rate for GCN.')\n",
    "    # GAT\n",
    "    parser.add_argument('--gat', action='store_true',\n",
    "                        help='GAT')\n",
    "    parser.add_argument('--gat_our', action='store_true',\n",
    "                        help='GAT_our')\n",
    "    parser.add_argument('--gat_attention_type', type = str, choices=['linear','dotprod','gcn'], default='dotprod',\n",
    "                        help='The attention used for gat')\n",
    "\n",
    "    parser.add_argument('--embedding_type', type=str,default='glove', choices=['glove','bert'])\n",
    "    parser.add_argument('--embedding_dim', type=int, default=300,\n",
    "                        help='Dimension of glove embeddings')\n",
    "    parser.add_argument('--dep_relation_embed_dim', type=int, default=300,\n",
    "                        help='Dimension for dependency relation embeddings.')\n",
    "\n",
    "    parser.add_argument('--hidden_size', type=int, default=300,\n",
    "                        help='Hidden size of bilstm, in early stage.')\n",
    "    parser.add_argument('--final_hidden_size', type=int, default=300,\n",
    "                        help='Hidden size of bilstm, in early stage.')\n",
    "    parser.add_argument('--num_mlps', type=int, default=2,\n",
    "                        help='Number of mlps in the last of model.')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=16, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--per_gpu_eval_batch_size\", default=32, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-3, type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    \n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                        help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                        help=\"Epsilon for Adam optimizer.\")\n",
    "\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=30.0, type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                        help=\"If > 0: set total number of training steps(that update the weights) to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=50,\n",
    "                        help=\"Log every X updates steps.\")\n",
    "    \n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def check_args(args):\n",
    "    '''\n",
    "    eliminate confilct situations\n",
    "    \n",
    "    '''\n",
    "    logger.info(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/07/2021 06:53:54 - INFO - __main__ -   {'dataset_name': 'rest', 'output_dir': 'output/r-gat', 'num_classes': 3, 'cuda_id': '3', 'seed': 2019, 'glove_dir': 'glove', 'bert_model_dir': '/data1/SHENWZH/models/bert_base', 'pure_bert': False, 'gat_bert': False, 'highway': True, 'num_layers': 2, 'add_non_connect': True, 'multi_hop': True, 'max_hop': 4, 'num_heads': 7, 'dropout': 0.8, 'num_gcn_layers': 1, 'gcn_mem_dim': 300, 'gcn_dropout': 0.2, 'gat': False, 'gat_our': True, 'gat_attention_type': 'dotprod', 'embedding_type': 'glove', 'embedding_dim': 300, 'dep_relation_embed_dim': 300, 'hidden_size': 300, 'final_hidden_size': 300, 'num_mlps': 2, 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 32, 'gradient_accumulation_steps': 2, 'learning_rate': 0.001, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 30.0, 'max_steps': -1, 'logging_steps': 50}\n",
      "07/07/2021 06:53:54 - INFO - __main__ -   Device is cuda\n",
      "07/07/2021 06:53:55 - INFO - datasets -   # Read rest Train set: 1978\n",
      "07/07/2021 06:53:55 - INFO - datasets -   # Read rest Test set: 600\n",
      "07/07/2021 06:53:55 - INFO - datasets -   *** Start processing data(unrolling and reshaping) ***\n",
      "07/07/2021 06:53:55 - INFO - datasets -   Total sentiment counter: defaultdict(<class 'int'>, {'negative': 805, 'positive': 2164, 'neutral': 633})\n",
      "07/07/2021 06:53:55 - INFO - datasets -   Multi-Aspect-Multi-Sentiment counter: defaultdict(<class 'int'>, {'positive': 351, 'neutral': 288, 'negative': 301})\n",
      "07/07/2021 06:53:55 - INFO - datasets -   *** Start processing data(unrolling and reshaping) ***\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Total sentiment counter: defaultdict(<class 'int'>, {'positive': 728, 'neutral': 196, 'negative': 196})\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Multi-Aspect-Multi-Sentiment counter: defaultdict(<class 'int'>, {'positive': 85, 'neutral': 83, 'negative': 60})\n",
      "07/07/2021 06:53:56 - INFO - datasets -   ****** After unrolling ******\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Train set size: 3602\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Test set size: 1120,\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Loading word vocab from output/r-gat/pkls/cached_rest_glove_word_vocab.pkl\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Loading word vecs from output/r-gat/pkls/cached_rest_glove_word_vecs.pkl\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Loading vocab of dependency tags from output/r-gat/pkls/cached_rest_dep_tag_vocab.pkl\n",
      "07/07/2021 06:53:56 - INFO - datasets -   Loading vocab of dependency tags from output/r-gat/pkls/cached_rest_pos_tag_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "    \n",
    "# Parse args\n",
    "args = parse_args(['--gat_our', '--highway', '--num_heads', '7', '--dropout', '0.8', '--output_dir',\n",
    "                   'output/r-gat', '--glove_dir', 'glove', '--cuda_id', '0'])\n",
    "check_args(args)\n",
    "\n",
    "# Setup CUDA, GPU training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_id\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.device = device\n",
    "logger.info('Device is %s', args.device)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Bert, load pretrained model and tokenizer, check if neccesary to put bert here\n",
    "if args.embedding_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model_dir)\n",
    "    args.tokenizer = tokenizer\n",
    "\n",
    "# Load datasets and vocabs\n",
    "train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab= load_datasets_and_vocabs(args)\n",
    "\n",
    "# Build Model\n",
    "# model = Aspect_Text_Multi_Syntax_Encoding(args, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "if args.pure_bert:\n",
    "    model = Pure_Bert(args)\n",
    "elif args.gat_bert:\n",
    "    model = Aspect_Bert_GAT(args, dep_tag_vocab['len'], pos_tag_vocab['len'])  # R-GAT + Bert\n",
    "elif args.gat_our:\n",
    "    model = Aspect_Text_GAT_ours(args, dep_tag_vocab['len'], pos_tag_vocab['len']) # R-GAT with reshaped tree\n",
    "else:\n",
    "    model = Aspect_Text_GAT_only(args, dep_tag_vocab['len'], pos_tag_vocab['len'])  # original GAT with reshaped tree\n",
    "\n",
    "model.to(args.device)\n",
    "# Train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "_, _,  all_eval_results = train(args, train_dataset, model, test_dataset)\n",
    "\n",
    "if len(all_eval_results):\n",
    "    best_eval_result = max(all_eval_results, key=lambda x: x['acc']) \n",
    "    for key in sorted(best_eval_result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(best_eval_result[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
