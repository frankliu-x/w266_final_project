{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1631: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\\'\\m\\not\\sure\\,\\this\\can\\work\\,\\lo\\##l\\-\\.\\-\n",
      "torch.Size([1, 15])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1559,  0.0427,  0.2496,  ..., -0.3521,  0.1700,  0.4932],\n",
      "         [ 0.3662,  0.3600,  0.6951,  ..., -0.4676,  0.1914,  0.3500],\n",
      "         [ 0.3161,  0.4953,  0.3921,  ..., -0.5428,  0.2684,  1.3029],\n",
      "         ...,\n",
      "         [ 0.4199,  0.1679,  0.5940,  ..., -0.1377,  0.1986,  0.4219],\n",
      "         [ 0.1325, -0.1673,  0.6549,  ..., -0.0030,  0.5450,  0.3319],\n",
      "         [ 0.3377, -0.0386,  1.0407,  ..., -0.0521,  0.5324,  0.7737]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-6.3580e-01,  2.9413e-03,  6.0078e-01,  3.9510e-01, -3.8253e-01,\n",
      "         -3.5963e-02,  4.5680e-01,  1.1072e-01,  8.0616e-01, -9.7523e-01,\n",
      "          6.8483e-01, -2.9353e-01,  9.5688e-01, -6.6352e-01,  9.3670e-01,\n",
      "         -4.7479e-02,  1.2822e-01, -3.6295e-01,  6.7251e-02, -1.9408e-01,\n",
      "          7.5842e-01,  7.7572e-01,  7.3900e-01,  5.8289e-02,  1.0056e-01,\n",
      "         -2.7342e-01,  4.0875e-03,  9.5002e-01,  9.0665e-01,  7.2732e-01,\n",
      "         -4.2145e-01,  1.1707e-01, -9.8805e-01, -3.3996e-02,  5.2755e-02,\n",
      "         -9.5016e-01, -8.5460e-02, -3.4941e-01, -1.3024e-01,  1.6734e-01,\n",
      "         -8.9847e-01,  1.3318e-02,  9.5546e-01, -5.7707e-01,  3.9303e-01,\n",
      "         -8.3544e-02, -9.4225e-01, -5.8858e-02, -9.0010e-01, -8.5180e-01,\n",
      "         -6.1983e-01, -7.1089e-01, -1.3615e-01,  4.7691e-02,  1.3469e-02,\n",
      "          3.8435e-01, -2.1589e-01,  7.3427e-02,  8.9797e-02, -1.6480e-01,\n",
      "         -1.9355e-01,  2.8070e-01,  3.2171e-01, -7.6324e-01, -8.5959e-01,\n",
      "         -8.1831e-01, -6.8631e-02, -4.9101e-02,  1.2187e-01, -8.4391e-02,\n",
      "          4.3431e-01,  8.0619e-02,  6.2524e-01, -8.9997e-01, -8.8820e-01,\n",
      "          3.4571e-02, -3.7880e-01,  9.9387e-01, -2.1113e-02, -9.7067e-01,\n",
      "          2.6474e-01, -8.3400e-01,  4.3025e-01,  9.1442e-01, -8.3708e-01,\n",
      "         -9.8587e-01, -2.0856e-01,  7.8606e-02, -9.8148e-01, -4.8839e-02,\n",
      "          2.3301e-01,  1.6960e-01, -4.2554e-01,  3.4315e-01,  4.2681e-01,\n",
      "         -1.3724e-01,  4.5849e-02,  6.0172e-01, -1.9855e-01,  1.5213e-01,\n",
      "          7.2227e-02, -2.3816e-02, -3.7546e-02, -9.2865e-02,  3.4119e-02,\n",
      "         -4.7565e-02, -4.5038e-01, -3.0659e-01, -6.6427e-01,  1.0150e-01,\n",
      "          2.4018e-01, -6.5131e-02,  4.6970e-04, -8.9657e-01,  6.1193e-02,\n",
      "         -1.0203e-02, -9.7173e-01, -3.0346e-01, -9.8822e-01,  5.7889e-01,\n",
      "         -1.4979e-02, -1.3545e-01,  9.4296e-01,  4.8034e-01, -5.8825e-02,\n",
      "         -3.2245e-02,  8.5932e-01, -9.9726e-01,  2.5760e-01, -3.3688e-01,\n",
      "          5.1167e-01,  3.9069e-03, -9.6712e-01, -9.6478e-01,  2.4758e-01,\n",
      "          8.8416e-01,  3.0905e-02,  9.5999e-01, -1.6108e-01,  9.3265e-01,\n",
      "          8.0809e-01,  3.8901e-01, -8.0934e-01, -1.4924e-01,  3.5477e-01,\n",
      "          2.8743e-02, -2.2570e-01, -1.5313e-01,  1.1582e-01, -5.4775e-01,\n",
      "         -3.1003e-02,  8.2001e-02,  7.4272e-01, -9.0351e-01, -8.6076e-02,\n",
      "          9.5415e-01,  3.7927e-01,  8.8699e-01,  8.9649e-01,  6.0068e-02,\n",
      "         -2.0415e-01,  6.5727e-01,  3.2907e-01,  1.3859e-01,  3.1757e-01,\n",
      "          2.3963e-01, -8.1279e-01,  1.5800e-01, -7.0211e-01,  6.8447e-01,\n",
      "          1.3369e-01, -4.1334e-02,  4.6923e-01, -9.6964e-01,  4.9504e-02,\n",
      "          1.6995e-01,  9.8058e-01,  5.9706e-01,  2.1113e-02, -7.0373e-01,\n",
      "         -1.1126e-01, -3.5033e-01, -9.4444e-01,  9.6746e-01,  3.0790e-02,\n",
      "          6.7356e-02,  1.4804e-01, -5.3432e-01, -7.6829e-01, -5.6049e-01,\n",
      "          3.5797e-01,  4.4604e-01, -7.5589e-01, -5.4155e-03, -2.4488e-01,\n",
      "         -1.4165e-01, -9.2276e-02,  1.6979e-01, -1.1465e-01, -2.2283e-01,\n",
      "         -2.4460e-01,  9.1546e-01,  4.0854e-01,  5.3264e-01, -6.4742e-01,\n",
      "          2.8017e-01, -8.7625e-01, -5.8571e-01, -5.4276e-03, -6.3765e-02,\n",
      "          1.8759e-02,  9.8033e-01, -2.1003e-01,  9.9272e-02, -7.8353e-01,\n",
      "         -9.7641e-01, -1.7351e-01, -8.1088e-01,  1.2039e-01, -2.3038e-01,\n",
      "          8.1891e-02,  2.5163e-01, -8.9607e-01,  2.5559e-01, -8.3183e-01,\n",
      "         -7.0472e-01,  4.6435e-02, -1.0817e-01,  8.5979e-02, -1.5818e-02,\n",
      "          9.0743e-01, -4.2174e-01, -3.4651e-01,  1.4098e-01,  9.5556e-01,\n",
      "          1.6152e-01, -7.8721e-01,  6.4939e-01,  7.7938e-02,  4.1888e-01,\n",
      "         -2.3616e-01,  8.7558e-01, -4.0201e-01,  2.1811e-01, -8.8029e-01,\n",
      "          4.8188e-01, -1.2760e-01,  8.4824e-01, -5.1509e-02, -8.8012e-01,\n",
      "         -8.9582e-01,  3.4935e-01, -4.5217e-02,  8.8421e-01, -6.6696e-02,\n",
      "          5.5220e-01, -8.9081e-01, -9.5855e-01, -4.9613e-01,  5.5008e-01,\n",
      "         -9.8490e-01, -2.1023e-02,  1.0318e-01,  1.5734e-01, -1.1100e-01,\n",
      "         -9.1934e-02, -9.5122e-01,  3.2460e-01,  4.6819e-02,  7.6225e-01,\n",
      "         -4.9525e-03, -5.0058e-01,  5.3607e-01, -9.4723e-01, -8.4684e-02,\n",
      "          7.1818e-02,  9.0650e-01, -2.4014e-01, -9.1914e-01,  2.2066e-01,\n",
      "          5.0000e-01,  1.1443e-01,  8.5516e-01,  7.8051e-01,  9.3463e-01,\n",
      "          9.4697e-01,  8.4163e-01,  3.9079e-01, -7.3304e-01, -1.1764e-01,\n",
      "          9.8199e-01,  2.1661e-01, -9.8636e-01, -8.6254e-01, -5.2975e-02,\n",
      "          3.6119e-01, -9.9600e-01, -8.1727e-02,  1.0323e-01, -8.3294e-01,\n",
      "         -7.7854e-01,  9.3090e-01,  7.7730e-01, -9.9342e-01,  3.9987e-01,\n",
      "          8.9200e-01, -3.3456e-01, -5.8041e-01, -1.0169e-01,  9.4682e-01,\n",
      "         -1.6360e-01,  3.7405e-01, -8.6026e-02,  2.4017e-01,  4.3961e-01,\n",
      "         -5.7442e-01,  7.5300e-01,  3.9082e-01,  1.4411e-01,  3.8331e-03,\n",
      "         -4.1512e-01, -8.8160e-01,  3.9288e-02, -1.0778e-02, -3.6999e-01,\n",
      "         -9.7286e-01, -7.3480e-02, -7.0792e-01,  2.7299e-01,  9.7908e-03,\n",
      "         -6.1465e-02, -5.0840e-01, -1.9238e-02, -6.1089e-01,  1.9016e-01,\n",
      "          4.1501e-01, -7.6034e-01, -3.4731e-01,  4.7661e-01, -7.9805e-01,\n",
      "          6.6586e-01, -9.6023e-01,  9.4640e-01, -2.0362e-01, -8.1913e-01,\n",
      "          9.9597e-01, -4.0373e-01, -8.6316e-01,  3.9019e-02, -7.2698e-02,\n",
      "         -5.7483e-01,  9.8770e-01,  1.8885e-01, -9.7004e-01, -3.9554e-01,\n",
      "         -2.0133e-01, -9.6662e-02, -2.3508e-01,  9.6710e-01,  1.8455e-01,\n",
      "          8.6361e-01,  4.0544e-01,  9.8529e-01, -9.8105e-01,  7.8743e-02,\n",
      "         -7.3064e-01, -9.4827e-01,  9.3179e-01,  9.5279e-01,  9.2464e-03,\n",
      "         -3.7094e-01, -1.7139e-01,  6.8887e-01,  1.0643e-01, -5.5539e-01,\n",
      "          1.7743e-01,  4.0637e-01, -2.7383e-02,  9.1592e-01, -8.8825e-02,\n",
      "         -3.6999e-01, -9.7616e-02,  5.8577e-01,  7.5274e-01,  3.5155e-02,\n",
      "          2.1896e-01, -1.0027e-01, -2.1246e-01, -7.6944e-02, -5.5110e-01,\n",
      "         -9.2250e-01,  3.1418e-01,  9.9431e-01,  3.6691e-01,  1.1091e-01,\n",
      "          7.9394e-01, -7.4040e-02, -9.0273e-02, -1.9634e-03,  1.5960e-01,\n",
      "          3.1354e-02, -3.9835e-01, -6.2598e-01, -6.3116e-01, -9.8447e-01,\n",
      "          6.5321e-01,  2.2341e-02, -4.4478e-02,  8.8703e-01, -4.4417e-01,\n",
      "         -8.6194e-02, -5.3139e-01, -6.8438e-01, -6.2362e-02,  1.8044e-03,\n",
      "         -9.3792e-01,  9.7029e-01,  1.0668e-02,  4.2211e-01,  5.0765e-01,\n",
      "          9.1948e-01, -4.0705e-01, -2.1170e-01, -1.4261e-01, -9.4193e-01,\n",
      "          3.0090e-02, -9.4755e-01,  9.6605e-01, -8.2195e-01,  1.4222e-01,\n",
      "         -4.0425e-02, -1.1899e-03,  9.8988e-01, -6.5530e-01,  4.2468e-01,\n",
      "          4.7316e-01,  5.9688e-01, -5.2019e-01, -6.8743e-01, -1.5851e-01,\n",
      "          2.5540e-01,  8.6616e-01, -1.3535e-01,  3.3670e-02, -9.2216e-01,\n",
      "         -8.7022e-01, -4.3056e-01, -6.5390e-01, -9.7393e-01,  7.5576e-01,\n",
      "         -1.0214e-01, -7.7251e-02, -5.2477e-01, -2.5503e-02, -5.2912e-01,\n",
      "         -6.0189e-01,  1.3504e-01, -9.2564e-01,  9.0571e-01, -1.0482e-01,\n",
      "          1.0685e-01, -1.9102e-02,  3.8658e-01, -8.0837e-01,  9.2721e-01,\n",
      "          4.1088e-01,  1.2469e-01, -1.8314e-01, -5.5401e-01,  4.3007e-01,\n",
      "         -5.6004e-01,  3.6123e-01,  2.1589e-02,  9.9571e-01, -3.2000e-01,\n",
      "         -4.1075e-01,  5.8611e-01,  3.1773e-01,  8.8218e-02,  2.9725e-01,\n",
      "         -3.4719e-02,  2.4550e-01,  8.5942e-01,  8.8440e-01,  3.9028e-01,\n",
      "         -1.6114e-02,  2.4024e-01, -1.9299e-02, -7.3450e-01,  8.2672e-01,\n",
      "         -2.2279e-01, -1.5252e-02,  1.7160e-01,  5.0529e-02,  5.5152e-01,\n",
      "          5.0602e-02,  9.7271e-02, -1.9263e-01, -4.4547e-02, -1.4872e-01,\n",
      "         -2.8319e-02,  9.8492e-01,  7.2868e-02, -2.1371e-01, -9.8142e-01,\n",
      "          3.5735e-01, -5.5243e-01,  8.8131e-01,  8.6154e-01, -3.0020e-01,\n",
      "          1.8112e-01, -1.5341e-01, -8.8517e-03,  1.9400e-01,  3.3589e-02,\n",
      "          2.6346e-02, -2.6193e-03,  9.6146e-02,  9.4027e-01, -1.8670e-01,\n",
      "         -9.6827e-01, -5.1254e-01,  3.7016e-02, -9.2084e-01,  6.8524e-01,\n",
      "         -1.6325e-01, -7.0259e-02, -5.1438e-02,  3.1040e-01, -1.4794e-01,\n",
      "         -4.3143e-02, -9.6403e-01, -7.5513e-02, -6.2293e-02,  9.6738e-01,\n",
      "         -1.5809e-02, -3.4473e-01, -8.9946e-01, -8.2873e-01, -3.4912e-01,\n",
      "          7.0511e-01, -8.8888e-01,  9.6818e-01, -9.5363e-01, -2.6124e-01,\n",
      "          9.6927e-01, -2.0222e-02, -8.8184e-01, -2.7621e-02, -9.7285e-02,\n",
      "          2.7423e-02,  3.1604e-01,  3.2024e-01, -9.5395e-01,  2.1190e-02,\n",
      "         -3.0965e-02,  3.3682e-03,  3.5472e-02, -2.7609e-02,  6.4095e-01,\n",
      "          2.2568e-01, -3.4456e-01, -1.5644e-01,  4.9351e-02,  8.3685e-02,\n",
      "          4.6638e-01, -9.1786e-02,  7.1748e-02,  1.7379e-03,  2.3665e-02,\n",
      "         -9.1943e-01,  8.5324e-02, -1.1873e-01, -7.5009e-01,  3.3933e-01,\n",
      "         -9.9430e-01, -3.8340e-01, -8.6619e-01, -1.4605e-01,  8.5873e-01,\n",
      "          2.1063e-01, -3.6488e-01, -7.2554e-01,  8.8269e-01,  9.6407e-01,\n",
      "          7.1781e-01, -1.4347e-01,  7.5352e-01, -7.5690e-01,  7.7814e-02,\n",
      "         -3.6689e-02,  1.1157e-01,  3.9919e-01,  5.3123e-01, -2.2263e-02,\n",
      "          9.9595e-01, -1.7106e-02, -1.9008e-01, -7.2911e-01,  5.1007e-02,\n",
      "         -4.4696e-02,  9.6264e-01, -1.1739e-01, -9.5438e-01,  2.9472e-01,\n",
      "         -3.1747e-01, -8.3539e-01,  1.2956e-01, -3.8908e-02, -1.2152e-01,\n",
      "          1.2308e-01,  9.3840e-01,  3.1925e-01, -3.9980e-01,  2.3702e-01,\n",
      "         -1.6191e-01, -5.2362e-02, -9.0845e-02, -6.0527e-01,  9.8034e-01,\n",
      "         -8.4003e-02,  4.6232e-01,  2.5376e-01, -1.0923e-01,  9.6312e-01,\n",
      "          2.0530e-01,  1.1149e-01,  4.0326e-03,  9.8490e-01,  2.1405e-01,\n",
      "         -9.1491e-01,  2.7287e-01, -9.0560e-01, -5.4772e-02, -8.4558e-01,\n",
      "          1.6313e-02,  1.0330e-01,  8.6263e-01,  3.5151e-02,  9.1287e-01,\n",
      "          8.0236e-01, -7.9998e-02,  6.1097e-01,  8.2468e-01,  1.1647e-01,\n",
      "         -9.2830e-01, -9.6746e-01, -9.7818e-01, -1.7481e-01, -7.5479e-02,\n",
      "          4.0821e-02,  8.5692e-03, -2.5731e-02, -6.9627e-02,  5.4685e-02,\n",
      "         -9.8512e-01,  8.6793e-01,  1.7722e-01, -7.6078e-01,  9.5690e-01,\n",
      "          1.1415e-01, -3.3210e-02, -8.9545e-02, -9.7143e-01, -7.4157e-01,\n",
      "          7.7706e-02, -7.7235e-03,  5.7381e-01,  1.9219e-01,  8.0473e-01,\n",
      "         -8.4500e-02, -1.5666e-01, -1.8386e-01,  5.4674e-01, -5.4873e-01,\n",
      "         -9.8537e-01,  2.2789e-01,  9.1657e-01, -5.2280e-01,  9.6647e-01,\n",
      "         -8.7914e-01, -6.8048e-02,  8.2380e-01,  3.2156e-01,  2.4492e-01,\n",
      "          4.9783e-01,  2.7522e-01, -1.2330e-01,  6.2778e-01,  8.6213e-01,\n",
      "          6.5534e-01,  9.7165e-01,  7.8135e-01,  4.1175e-01,  7.6813e-01,\n",
      "         -5.2839e-03,  7.7198e-01, -8.8773e-01, -6.0284e-02,  2.1722e-01,\n",
      "          2.2019e-01,  1.2889e-01,  4.7955e-03, -6.6512e-01,  4.5565e-01,\n",
      "         -8.4128e-02,  2.6312e-01, -2.0529e-01,  8.5974e-02, -1.7485e-01,\n",
      "         -7.6450e-02, -4.1549e-01,  9.1318e-02,  4.8451e-01,  1.0533e-02,\n",
      "          9.1728e-01, -1.3689e-01,  8.7594e-02, -1.1733e-01,  2.1779e-01,\n",
      "          8.6870e-01, -8.7354e-01,  4.9768e-01,  9.6423e-02,  8.7152e-01,\n",
      "         -8.6767e-02, -4.6757e-02,  7.0632e-01, -7.7771e-01,  1.1362e-01,\n",
      "          7.7681e-02, -1.7228e-01,  5.8369e-01, -3.3969e-01, -1.9758e-01,\n",
      "         -2.0740e-01,  2.1051e-01,  2.6970e-02,  7.8795e-01,  6.7404e-01,\n",
      "          9.2224e-01, -3.1945e-02, -3.2871e-02,  2.3831e-01, -3.5757e-01,\n",
      "         -9.8720e-01,  1.1307e-01,  6.3256e-01, -5.0527e-01,  6.0221e-01,\n",
      "         -3.8249e-01,  5.4502e-01, -8.8142e-01, -9.8506e-02,  2.9458e-01,\n",
      "         -5.0064e-01, -4.8671e-02,  4.6152e-01,  3.5636e-01,  9.4074e-01,\n",
      "         -4.4408e-01,  8.1130e-01,  3.5628e-01,  8.4374e-01,  3.6409e-01,\n",
      "          4.0303e-01, -3.6239e-01,  8.2170e-01]], grad_fn=<TanhBackward>), hidden_states=(tensor([[[ 0.0890,  0.0295, -0.3723,  ...,  0.1924,  0.2647,  0.3449],\n",
      "         [-0.3326,  0.2762,  0.0828,  ...,  0.6663,  0.8544, -1.0239],\n",
      "         [ 0.0053,  0.1348,  0.2541,  ...,  0.2022,  0.2454, -0.3520],\n",
      "         ...,\n",
      "         [-0.0907,  0.2641,  0.3417,  ...,  0.6755,  0.6276,  0.6125],\n",
      "         [-0.5063,  0.3854,  0.0492,  ...,  0.4435,  0.5867,  0.7113],\n",
      "         [ 0.0571,  0.3530,  0.1322,  ...,  0.8536,  0.4883,  0.6924]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-2.2385e-03,  2.8265e-01, -3.2196e-01,  ...,  3.1062e-01,\n",
      "           1.6151e-01,  1.6790e-02],\n",
      "         [-2.6875e-01,  2.6596e-01, -4.7697e-01,  ...,  5.5671e-01,\n",
      "           7.8688e-01, -1.3078e+00],\n",
      "         [ 3.0366e-01,  4.1863e-01, -2.6112e-04,  ..., -3.5649e-01,\n",
      "           6.5947e-01, -5.1996e-01],\n",
      "         ...,\n",
      "         [ 6.8993e-01,  4.9268e-02,  1.5615e-01,  ...,  6.4383e-01,\n",
      "           9.9540e-01,  6.2567e-01],\n",
      "         [-3.8444e-01,  9.8312e-03, -5.7603e-01,  ...,  1.0698e-01,\n",
      "           3.0362e-01,  6.8244e-01],\n",
      "         [ 6.6631e-01,  1.8878e-01, -2.7614e-01,  ...,  9.0107e-01,\n",
      "           3.6685e-01,  7.3702e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1239, -0.0791, -0.2722,  ...,  0.4458,  0.4083, -0.1822],\n",
      "         [-0.2662,  0.2396,  0.1303,  ...,  1.0270,  1.0323, -1.6903],\n",
      "         [ 0.2357,  0.3797,  0.1779,  ...,  0.1151,  0.6962,  0.0306],\n",
      "         ...,\n",
      "         [ 0.9756,  0.5702,  0.8018,  ...,  0.9219,  1.6740,  0.2698],\n",
      "         [-0.2107, -0.2581, -0.4242,  ...,  0.0042,  0.3795,  0.8527],\n",
      "         [ 0.8269,  1.0054,  0.5928,  ...,  1.2985,  0.3304,  0.6254]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 1.7501e-01, -3.2946e-01, -9.8765e-02,  ...,  4.2023e-01,\n",
      "           3.5396e-01, -7.9676e-02],\n",
      "         [-5.1801e-02, -2.3003e-01,  3.7239e-01,  ...,  6.2871e-01,\n",
      "           7.4409e-01, -1.4961e+00],\n",
      "         [ 5.5199e-01,  1.4958e-02,  3.3119e-02,  ...,  3.6124e-01,\n",
      "           5.3093e-06,  2.1503e-01],\n",
      "         ...,\n",
      "         [ 7.1544e-01,  3.3235e-01,  1.0291e+00,  ...,  5.0314e-01,\n",
      "           1.4270e+00,  1.1054e-02],\n",
      "         [ 1.3521e-02, -5.3693e-01, -2.4046e-01,  ..., -1.5232e-01,\n",
      "           3.8752e-01,  8.5473e-01],\n",
      "         [ 1.1393e+00,  6.4342e-01,  1.0590e+00,  ...,  7.5066e-01,\n",
      "           1.9560e-01,  4.3973e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.4216, -0.5761, -0.4737,  ...,  0.3893,  0.1378,  0.2191],\n",
      "         [-0.1827, -0.6554,  0.3373,  ...,  0.7580,  0.5347, -1.0001],\n",
      "         [ 0.9138, -0.3404, -0.2800,  ...,  0.2009,  0.4459,  0.5353],\n",
      "         ...,\n",
      "         [ 0.7739,  0.0409,  0.7775,  ...,  0.7110,  0.7865,  0.0935],\n",
      "         [-0.1025, -0.9635,  0.9315,  ..., -0.1136,  0.0861,  0.9751],\n",
      "         [ 0.6822,  0.5032,  1.8941,  ...,  0.8332, -0.7397,  0.8307]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.3394, -0.4838, -0.0298,  ...,  0.1325,  0.5446,  0.5341],\n",
      "         [ 0.1105, -0.5266,  0.5979,  ...,  0.3251, -0.1082, -0.4771],\n",
      "         [ 0.9028, -0.1316, -0.9456,  ..., -0.0561,  0.4523,  0.2826],\n",
      "         ...,\n",
      "         [ 0.2927,  0.1395,  1.2182,  ...,  0.8828,  0.5892, -0.2313],\n",
      "         [-0.3779, -0.5677,  1.3360,  ..., -0.2168, -0.2279,  0.8371],\n",
      "         [-0.3416, -0.0999,  1.9261,  ...,  0.3227,  0.0530,  1.1776]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1480, -0.3261,  0.1455,  ..., -0.3811,  0.0083,  0.5384],\n",
      "         [-0.1010, -0.4217,  0.5822,  ..., -0.3006, -0.3402, -0.4920],\n",
      "         [ 0.8315, -0.1196, -0.6081,  ..., -0.4520,  0.5421,  0.7754],\n",
      "         ...,\n",
      "         [ 0.2407,  0.1755,  1.3579,  ...,  0.7481,  0.3505, -0.3285],\n",
      "         [-0.1064, -0.2686,  1.1202,  ..., -0.2468, -0.2147,  0.8523],\n",
      "         [-0.3927,  0.1947,  1.7985,  ...,  0.8110,  0.0193,  0.6975]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.1193, -0.2367,  0.5228,  ..., -0.2477,  0.0967,  0.4951],\n",
      "         [-0.2975, -0.1667,  1.0266,  ..., -0.1697,  0.3079, -0.0314],\n",
      "         [ 0.6932,  0.2408, -0.3393,  ..., -0.4190,  0.8449,  0.9276],\n",
      "         ...,\n",
      "         [ 0.1140,  0.2452,  1.5473,  ...,  0.7541,  0.9764, -0.3063],\n",
      "         [-0.4154, -0.4515,  1.1467,  ...,  0.0038,  0.7863,  0.6312],\n",
      "         [-0.5585,  0.2953,  2.0262,  ...,  0.7364,  0.5440,  0.8209]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 2.3232e-01, -2.2506e-01,  6.4849e-01,  ...,  7.5594e-02,\n",
      "           2.2583e-02,  3.6806e-01],\n",
      "         [-1.0812e-03,  2.5227e-01,  1.2600e+00,  ...,  5.0263e-01,\n",
      "           2.8879e-01, -1.4412e-01],\n",
      "         [ 8.8916e-01,  9.0746e-01, -2.8268e-01,  ..., -1.5094e-01,\n",
      "           7.1491e-01,  9.6748e-01],\n",
      "         ...,\n",
      "         [ 3.8051e-01, -1.7441e-01,  1.4951e+00,  ...,  9.3636e-01,\n",
      "           3.9603e-01, -1.0286e-01],\n",
      "         [-3.2658e-01, -2.1213e-01,  1.2889e+00,  ...,  5.0388e-01,\n",
      "           2.6389e-01,  5.8130e-01],\n",
      "         [-3.6007e-01, -2.7467e-01,  2.3979e+00,  ...,  7.0780e-01,\n",
      "           2.3552e-01,  6.2487e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.4937, -0.1987,  0.6313,  ...,  0.0343, -0.1474,  0.0930],\n",
      "         [-0.0361,  0.3367,  1.3058,  ..., -0.0520,  0.1937, -0.4364],\n",
      "         [ 0.8056,  1.1586, -0.3845,  ..., -0.4611,  0.7230,  0.6945],\n",
      "         ...,\n",
      "         [ 0.8789, -0.0550,  1.2449,  ...,  0.6535,  0.4532, -0.3018],\n",
      "         [ 0.1901, -0.2685,  1.1303,  ...,  0.2535,  0.1150,  0.0033],\n",
      "         [ 0.5046, -0.2759,  1.9358,  ...,  0.4342,  0.1385,  0.5656]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.4263, -0.4328,  0.7582,  ...,  0.1443, -0.4910,  0.6900],\n",
      "         [ 0.4041,  0.0661,  1.3532,  ..., -0.1326,  0.1023,  0.1834],\n",
      "         [ 0.6524,  0.4273, -0.1743,  ..., -0.3957,  0.1363,  1.5538],\n",
      "         ...,\n",
      "         [ 0.9648, -0.1656,  1.3937,  ...,  0.8441,  0.3077,  0.0756],\n",
      "         [ 0.4259, -0.1855,  1.2323,  ...,  0.4488,  0.3498,  0.1701],\n",
      "         [ 0.7844, -0.4138,  1.9178,  ...,  0.7546,  0.2182,  0.8858]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.5355, -0.2579,  0.7328,  ..., -0.3828, -0.5355,  0.8025],\n",
      "         [ 0.7422,  0.1590,  1.6009,  ..., -0.6867, -0.1103,  0.6380],\n",
      "         [ 0.5548,  0.2401,  0.2969,  ..., -0.6947, -0.0569,  1.9620],\n",
      "         ...,\n",
      "         [ 0.8411,  0.1652,  1.1451,  ...,  0.1495, -0.0502,  0.4577],\n",
      "         [ 0.3499, -0.3973,  1.1223,  ...,  0.0447,  0.3865,  0.0269],\n",
      "         [ 0.6197, -0.2216,  1.7331,  ...,  0.1161,  0.3965,  0.9370]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1559,  0.0427,  0.2496,  ..., -0.3521,  0.1700,  0.4932],\n",
      "         [ 0.3662,  0.3600,  0.6951,  ..., -0.4676,  0.1914,  0.3500],\n",
      "         [ 0.3161,  0.4953,  0.3921,  ..., -0.5428,  0.2684,  1.3029],\n",
      "         ...,\n",
      "         [ 0.4199,  0.1679,  0.5940,  ..., -0.1377,  0.1986,  0.4219],\n",
      "         [ 0.1325, -0.1673,  0.6549,  ..., -0.0030,  0.5450,  0.3319],\n",
      "         [ 0.3377, -0.0386,  1.0407,  ..., -0.0521,  0.5324,  0.7737]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./test/saved_model/vocab.txt')\n",
    "\n",
    "bert = BertModel.from_pretrained('./test/saved_model/')\n",
    "\n",
    "s = \"I'm not sure, this can work, lol -.-\"\n",
    "\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(\"\\\\\".join(tokens))\n",
    "# \"i\\\\'\\\\m\\\\not\\\\sure\\\\,\\\\this\\\\can\\\\work\\\\,\\\\lo\\\\##l\\\\-\\\\.\\\\-\"\n",
    "# 是否需要这样做？\n",
    "# tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)])\n",
    "print(ids.shape)\n",
    "# torch.Size([1, 15])\n",
    "\n",
    "result = bert(ids, output_hidden_states=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(result[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768]),\n",
       " torch.Size([1, 15, 768])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in result[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1559,  0.0427,  0.2496,  ..., -0.3521,  0.1700,  0.4932],\n",
      "         [ 0.3662,  0.3600,  0.6951,  ..., -0.4676,  0.1914,  0.3500],\n",
      "         [ 0.3161,  0.4953,  0.3921,  ..., -0.5428,  0.2684,  1.3029],\n",
      "         ...,\n",
      "         [ 0.4199,  0.1679,  0.5940,  ..., -0.1377,  0.1986,  0.4219],\n",
      "         [ 0.1325, -0.1673,  0.6549,  ..., -0.0030,  0.5450,  0.3319],\n",
      "         [ 0.3377, -0.0386,  1.0407,  ..., -0.0521,  0.5324,  0.7737]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(result[2][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1559,  0.0427,  0.2496,  ..., -0.3521,  0.1700,  0.4932],\n",
      "         [ 0.3662,  0.3600,  0.6951,  ..., -0.4676,  0.1914,  0.3500],\n",
      "         [ 0.3161,  0.4953,  0.3921,  ..., -0.5428,  0.2684,  1.3029],\n",
      "         ...,\n",
      "         [ 0.4199,  0.1679,  0.5940,  ..., -0.1377,  0.1986,  0.4219],\n",
      "         [ 0.1325, -0.1673,  0.6549,  ..., -0.0030,  0.5450,  0.3319],\n",
      "         [ 0.3377, -0.0386,  1.0407,  ..., -0.0521,  0.5324,  0.7737]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "print(result[2][12] == result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Frozen Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert.named_parameters():                                            \n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in bert.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(bert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
